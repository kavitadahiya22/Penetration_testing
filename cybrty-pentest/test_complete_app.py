#!/usr/bin/env python3
"""
Complete Application Test Suite for Cybrty Pentesting Service
Tests all components: Configuration, Database Models, CrewAI, Tools, API
"""

import asyncio
import json
import traceback
from datetime import datetime
from typing import Dict, Any, Optional

def print_test_header(test_name: str):
    """Print a formatted test header."""
    print(f"\n{'='*60}")
    print(f"üß™ TESTING: {test_name}")
    print(f"{'='*60}")

def print_success(message: str):
    """Print success message."""
    print(f"‚úÖ {message}")

def print_error(message: str, error: Optional[Exception] = None):
    """Print error message."""
    print(f"‚ùå {message}")
    if error:
        print(f"   Error: {str(error)}")
        print(f"   Traceback: {traceback.format_exc()}")

def print_info(message: str):
    """Print info message."""
    print(f"‚ÑπÔ∏è  {message}")

def test_configuration():
    """Test configuration loading and validation."""
    print_test_header("Configuration System")
    
    try:
        from app.core.config import get_settings, Settings
        settings = get_settings()
        
        # Test basic config structure
        assert hasattr(settings, 'server'), "Missing server config"
        assert hasattr(settings, 'model'), "Missing model config"
        assert hasattr(settings, 'opensearch'), "Missing opensearch config"
        assert hasattr(settings, 'tools'), "Missing tools config"
        assert hasattr(settings, 'policy'), "Missing policy config"
        assert hasattr(settings, 'artifacts'), "Missing artifacts config"
        
        print_success(f"Configuration loaded successfully")
        print_info(f"Server: {settings.server.host}:{settings.server.port}")
        print_info(f"Model Provider: {settings.model.provider}")
        print_info(f"Artifacts Dir: {settings.artifacts.dir}")
        print_info(f"Allowed Networks: {settings.policy.allow_networks}")
        
        return True
        
    except Exception as e:
        print_error("Configuration test failed", e)
        return False

def test_crewai_components():
    """Test CrewAI agents, roles, and registry."""
    print_test_header("CrewAI Components")
    
    try:
        from app.core.crew.roles import AgentRoles
        from app.core.crew.tasks import TaskTemplates
        from app.core.crew.registry import CrewRegistry
        from app.core.config import get_settings
        
        # Test agent roles
        roles = AgentRoles.get_all_roles()
        print_success(f"Agent roles loaded: {list(roles.keys())}")
        
        # Test task templates
        templates = TaskTemplates()
        recon_task = templates.create_recon_task(
            targets=["192.168.1.0/24"],
            params={"profile": "-sV -T4"}
        )
        print_success(f"Task template created: {recon_task['description'][:50]}...")
        
        # Test registry
        settings = get_settings()
        registry = CrewRegistry(settings)
        print_success(f"Registry initialized with agents: {list(registry._agents.keys())}")
        
        # Test crew creation
        crew = registry.create_simple_crew(['recon', 'web'])
        print_success(f"Crew created with {len(crew.agents)} agents")
        
        # Test agent details
        for agent in crew.agents:
            print_info(f"Agent: {agent.role} - Tools: {len(agent.tools)} - Delegation: {agent.allow_delegation}")
        
        return True
        
    except Exception as e:
        print_error("CrewAI components test failed", e)
        return False

def test_tools_and_utilities():
    """Test pentesting tools and utilities."""
    print_test_header("Tools and Utilities")
    
    try:
        from app.tools import NmapAgent, NmapTool
        from app.tools.crewai_utils import AsyncShellRunner, ArtifactManager, OutputParser, validate_network_target
        from app.core.config import get_settings
        
        settings = get_settings()
        
        # Test utilities
        shell_runner = AsyncShellRunner()
        artifact_manager = ArtifactManager(settings.artifacts.dir)
        parser = OutputParser()
        print_success("Utility classes initialized successfully")
        
        # Test network validation
        valid_target = validate_network_target("192.168.1.1", ["192.168.0.0/16"])
        invalid_target = validate_network_target("8.8.8.8", ["192.168.0.0/16"])
        print_success(f"Network validation working: valid={valid_target}, invalid={invalid_target}")
        
        # Test NmapTool creation
        nmap_tool = NmapTool(settings)
        print_success(f"NmapTool created: {nmap_tool.name}")
        print_info(f"Tool description: {nmap_tool.description[:100]}...")
        
        # Test legacy NmapAgent
        nmap_agent = NmapAgent()
        print_success(f"NmapAgent created with binary: {nmap_agent.binary_path}")
        
        return True
        
    except Exception as e:
        print_error("Tools and utilities test failed", e)
        return False

async def test_async_operations():
    """Test asynchronous operations."""
    print_test_header("Async Operations")
    
    try:
        from app.tools.crewai_utils import AsyncShellRunner
        
        shell_runner = AsyncShellRunner()
        
        # Test simple command
        returncode, stdout, stderr = await shell_runner.run_command(["echo", "Hello CrewAI"], timeout=5)
        print_success(f"Async command executed: returncode={returncode}")
        print_info(f"Output: {stdout.strip()}")
        
        # Test command with error
        returncode, stdout, stderr = await shell_runner.run_command(["ls", "/nonexistent"], timeout=5)
        print_success(f"Error handling works: returncode={returncode}")
        print_info(f"Error: {stderr.strip()}")
        
        return True
        
    except Exception as e:
        print_error("Async operations test failed", e)
        return False

def test_api_models():
    """Test API request/response models."""
    print_test_header("API Models")
    
    try:
        from app.api.schemas import (
            PlanRequest, PlanResponse, RunRequest, RunResponse,
            Step, RunInputs
        )
        
        # Test PlanRequest
        plan_req = PlanRequest(
            targets=["https://example.com"],
            depth="medium",
            features=["recon", "web"],
            simulate=True,
            tenant_id="test-tenant"
        )
        print_success(f"PlanRequest created with {len(plan_req.targets)} targets")
        
        # Test RunInputs and RunRequest
        run_inputs = RunInputs(
            targets=["192.168.1.0/24"],
            depth="standard", 
            features=["recon"],
            simulate=True
        )
        
        run_req = RunRequest(
            tenant_id="test-tenant",
            inputs=run_inputs,
            plan_id="test-plan-001"
        )
        print_success(f"RunRequest created: plan_id={run_req.plan_id}")
        
        # Test Step model
        step = Step(
            id="step-001",
            agent="recon",
            tool="nmap",
            params={"target": "192.168.1.1"}
        )
        print_success(f"Step created: {step.agent} using {step.tool}")
        
        return True
        
    except Exception as e:
        print_error("API models test failed", e)
        return False

def test_opensearch_integration():
    """Test OpenSearch index mappings and logging preparation."""
    print_test_header("OpenSearch Integration")
    
    try:
        import json
        from pathlib import Path
        
        # Test index mappings file
        mappings_file = Path("config/samples/opensearch_index_mappings.json")
        if mappings_file.exists():
            with open(mappings_file, 'r') as f:
                mappings = json.load(f)
            
            required_indices = ["cybrty-actions", "cybrty-runs", "cybrty-planner"]
            for index_name in required_indices:
                assert index_name in mappings, f"Missing index: {index_name}"
                assert "mappings" in mappings[index_name], f"Missing mappings for {index_name}"
            
            print_success("OpenSearch index mappings validated")
            print_info(f"Available indices: {list(mappings.keys())}")
            
            # Test specific field mappings
            actions_props = mappings["cybrty-actions"]["mappings"]["properties"]
            required_fields = ["run_id", "step_id", "agent", "tool", "status"]
            for field in required_fields:
                assert field in actions_props, f"Missing field {field} in cybrty-actions"
            
            print_success("Required fields validated in index mappings")
        else:
            print_error("OpenSearch index mappings file not found")
            return False
        
        return True
        
    except Exception as e:
        print_error("OpenSearch integration test failed", e)
        return False

def test_artifact_management():
    """Test artifact creation and management."""
    print_test_header("Artifact Management")
    
    try:
        from app.tools.crewai_utils import ArtifactManager
        from app.core.config import get_settings
        import asyncio
        
        settings = get_settings()
        artifact_manager = ArtifactManager(settings.artifacts.dir)
        
        async def test_artifacts():
            # Test artifact creation
            test_content = "Test scan results\nHost: 192.168.1.1\nPorts: 80, 443"
            artifact_path = await artifact_manager.save_artifact(
                "test-run-001", "nmap_scan.txt", test_content
            )
            print_success(f"Artifact saved: {artifact_path}")
            
            # Test directory creation
            run_dir = artifact_manager.create_run_directory("test-run-002")
            print_success(f"Run directory created: {run_dir}")
            
            # Test filename sanitization
            safe_name = artifact_manager.sanitize_filename("test<>file?.txt")
            print_success(f"Filename sanitized: {safe_name}")
            
            return True
        
        result = asyncio.run(test_artifacts())
        return result
        
    except Exception as e:
        print_error("Artifact management test failed", e)
        return False

def test_security_policies():
    """Test security policy enforcement."""
    print_test_header("Security Policies")
    
    try:
        from app.tools.crewai_utils import validate_network_target
        from app.core.config import get_settings
        
        settings = get_settings()
        allowed_networks = settings.policy.allow_networks
        
        # Test allowed targets
        test_cases = [
            ("192.168.1.1", True),      # Should be allowed
            ("10.0.0.1", True),         # Should be allowed  
            ("8.8.8.8", False),         # Should be blocked
            ("172.16.1.1", False),      # Should be blocked (not in allowed list)
            ("192.168.0.0/24", True),   # Network range should be allowed
        ]
        
        for target, should_be_allowed in test_cases:
            result = validate_network_target(target, allowed_networks)
            if result == should_be_allowed:
                print_success(f"Target {target}: {'allowed' if result else 'blocked'} (correct)")
            else:
                print_error(f"Target {target}: {'allowed' if result else 'blocked'} (incorrect - should be {'allowed' if should_be_allowed else 'blocked'})")
                return False
        
        # Test host count limits
        max_hosts = settings.policy.max_host_count
        print_success(f"Host count limit: {max_hosts}")
        
        return True
        
    except Exception as e:
        print_error("Security policies test failed", e)
        return False

def run_comprehensive_tests():
    """Run all tests and provide summary."""
    print(f"\nüöÄ STARTING COMPREHENSIVE APPLICATION TESTS")
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}")
    
    tests = [
        ("Configuration System", test_configuration),
        ("CrewAI Components", test_crewai_components),
        ("Tools and Utilities", test_tools_and_utilities),
        ("Async Operations", lambda: asyncio.run(test_async_operations())),
        ("API Models", test_api_models),
        ("OpenSearch Integration", test_opensearch_integration),
        ("Artifact Management", test_artifact_management),
        ("Security Policies", test_security_policies),
    ]
    
    results = {}
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results[test_name] = result
            if result:
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print_error(f"Test {test_name} crashed", e)
            results[test_name] = False
            failed += 1
    
    # Print summary
    print_test_header("TEST SUMMARY")
    print(f"Total Tests: {len(tests)}")
    print(f"‚úÖ Passed: {passed}")
    print(f"‚ùå Failed: {failed}")
    print(f"Success Rate: {(passed/len(tests)*100):.1f}%")
    
    print(f"\nDetailed Results:")
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"  {status} {test_name}")
    
    if failed == 0:
        print(f"\nüéâ ALL TESTS PASSED! The application is ready for deployment.")
    else:
        print(f"\n‚ö†Ô∏è  {failed} test(s) failed. Please review and fix issues before deployment.")
    
    return failed == 0

if __name__ == "__main__":
    success = run_comprehensive_tests()
    exit(0 if success else 1)
