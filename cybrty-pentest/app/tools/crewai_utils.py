"""Utility functions for CrewAI tool integration."""

import asyncio
import json
import os
import subprocess
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from xml.etree import ElementTree as ET

import aiofiles
import structlog

logger = structlog.get_logger(__name__)


class AsyncShellRunner:
    """Async shell command runner with timeout and logging."""
    
    @staticmethod
    async def run_command(
        cmd: List[str],
        cwd: Optional[str] = None,
        timeout: int = 300,
        capture_output: bool = True
    ) -> Tuple[int, str, str]:
        """Run shell command asynchronously."""
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=subprocess.PIPE if capture_output else None,
                stderr=subprocess.PIPE if capture_output else None,
                cwd=cwd
            )
            
            stdout, stderr = await asyncio.wait_for(
                process.communicate(), 
                timeout=timeout
            )
            
            return (
                process.returncode if process.returncode is not None else -1,
                stdout.decode('utf-8', errors='ignore') if stdout else '',
                stderr.decode('utf-8', errors='ignore') if stderr else ''
            )
            
        except asyncio.TimeoutError:
            logger.error("Command timeout", cmd=cmd, timeout=timeout)
            if process:
                process.kill()
                await process.wait()
            return -1, '', f'Command timed out after {timeout} seconds'
            
        except Exception as e:
            logger.error("Command execution failed", cmd=cmd, error=str(e))
            return -1, '', str(e)


class ArtifactManager:
    """Manage artifacts with OpenSearch-only JSON storage."""
    
    def __init__(self, base_dir: str, opensearch_logger=None):
        """Initialize artifact manager."""
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.opensearch_logger = opensearch_logger
    
    def create_run_directory(self, run_id: str) -> Path:
        """Create directory for a specific run (only for non-JSON artifacts)."""
        run_dir = self.base_dir / run_id
        run_dir.mkdir(parents=True, exist_ok=True)
        return run_dir
    
    def get_artifact_path(self, run_id: str, filename: str) -> Path:
        """Get path for artifact file."""
        run_dir = self.create_run_directory(run_id)
        return run_dir / self.sanitize_filename(filename)
    
    @staticmethod
    def sanitize_filename(filename: str) -> str:
        """Sanitize filename for safe file operations."""
        import re
        # Remove or replace unsafe characters
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        filename = re.sub(r'[^\w\-_\.]', '_', filename)
        return filename[:255]  # Limit length
    
    async def save_artifact(
        self, 
        run_id: str, 
        filename: str, 
        content: str,
        mode: str = 'w',
        artifact_type: str = 'file'
    ) -> str:
        """Save artifact - JSON to OpenSearch, others to file system."""
        
        # DEBUG: Log entry parameters
        logger.debug("ArtifactManager.save_artifact called", 
                    run_id=run_id, filename=filename, artifact_type=artifact_type,
                    content_length=len(content) if content else 0,
                    opensearch_logger_available=bool(self.opensearch_logger))
        
        # Check if this is JSON content
        is_json_artifact = filename.endswith('.json') or artifact_type == 'json'
        
        # DEBUG: Log detection results
        logger.debug("JSON artifact detection", 
                    filename=filename, is_json_artifact=is_json_artifact,
                    filename_ends_with_json=filename.endswith('.json'),
                    artifact_type_is_json=(artifact_type == 'json'))
        
        if is_json_artifact and self.opensearch_logger:
            logger.info("Attempting to save JSON artifact to OpenSearch", 
                       run_id=run_id, filename=filename)
            
            # Store JSON artifacts only in OpenSearch
            try:
                # Parse content if it's a JSON string
                if isinstance(content, str):
                    try:
                        json_content = json.loads(content)
                        logger.debug("Successfully parsed JSON content", 
                                   filename=filename, keys=list(json_content.keys()) if isinstance(json_content, dict) else "non-dict")
                    except json.JSONDecodeError as parse_error:
                        logger.warning("Content is not valid JSON, falling back to file storage", 
                                     filename=filename, error=str(parse_error))
                        return await self._save_to_file(run_id, filename, content, mode)
                else:
                    json_content = content
                    logger.debug("Content is already parsed JSON", filename=filename)
                
                # Create artifact document for OpenSearch
                artifact_doc = {
                    "run_id": run_id,
                    "artifact_name": filename,
                    "artifact_type": "json",
                    "content": json_content,
                    "created_at": datetime.utcnow().isoformat(),
                    "@timestamp": datetime.utcnow().isoformat()
                }
                
                logger.debug("Attempting OpenSearch index operation", 
                           index="cybrty-artifacts", doc_keys=list(artifact_doc.keys()))
                
                # Store in OpenSearch artifacts index
                success = await self.opensearch_logger.client.index_document(
                    "cybrty-artifacts", artifact_doc
                )
                
                if success:
                    artifact_reference = f"opensearch://cybrty-artifacts/{run_id}/{filename}"
                    logger.info("JSON artifact saved to OpenSearch successfully", 
                              run_id=run_id, filename=filename, reference=artifact_reference)
                    return artifact_reference
                else:
                    logger.error("OpenSearch index_document returned False", 
                               run_id=run_id, filename=filename)
                    # Fallback to file system
                    return await self._save_to_file(run_id, filename, content, mode)
                    
            except Exception as e:
                logger.error("Exception during OpenSearch save operation", 
                           run_id=run_id, filename=filename, error=str(e), 
                           error_type=type(e).__name__)
                import traceback
                logger.debug("Full traceback", traceback=traceback.format_exc())
                # Fallback to file system
                return await self._save_to_file(run_id, filename, content, mode)
        else:
            # Log why we're not using OpenSearch
            if not is_json_artifact:
                logger.debug("Not a JSON artifact, using file storage", filename=filename)
            elif not self.opensearch_logger:
                logger.warning("OpenSearch logger not available, using file storage", filename=filename)
            
            # Save non-JSON artifacts to file system as before
            return await self._save_to_file(run_id, filename, content, mode)
    
    async def _save_to_file(self, run_id: str, filename: str, content: str, mode: str = 'w') -> str:
        """Save artifact to file system (fallback or non-JSON)."""
        filepath = self.get_artifact_path(run_id, filename)
        
        logger.debug("Saving artifact to file system", 
                    run_id=run_id, filename=filename, filepath=str(filepath), mode=mode)
        
        try:
            async with aiofiles.open(filepath, mode) as f:
                await f.write(content)
            
            logger.info("Artifact saved to file", path=str(filepath))
            return str(filepath)
            
        except Exception as e:
            logger.error("Failed to save artifact to file", path=str(filepath), error=str(e))
            raise
    
    async def get_json_artifact(self, run_id: str, filename: str) -> Optional[Dict[str, Any]]:
        """Retrieve JSON artifact from OpenSearch."""
        if not self.opensearch_logger:
            return None
            
        try:
            # Query OpenSearch for the artifact
            query = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"run_id": run_id}},
                            {"term": {"artifact_name": filename}},
                            {"term": {"artifact_type": "json"}}
                        ]
                    }
                }
            }
            
            response = await self.opensearch_logger.client.search("cybrty-artifacts", query)
            
            if response and response.get("hits", {}).get("hits"):
                artifact_doc = response["hits"]["hits"][0]["_source"]
                return artifact_doc.get("content")
            
            return None
            
        except Exception as e:
            logger.error("Error retrieving JSON artifact from OpenSearch", 
                       run_id=run_id, filename=filename, error=str(e))
            return None
    
    async def list_artifacts(self, run_id: str) -> Dict[str, Any]:
        """List all artifacts for a run (both OpenSearch and file system)."""
        artifacts = {
            "json_artifacts": [],
            "file_artifacts": []
        }
        
        # Get JSON artifacts from OpenSearch
        if self.opensearch_logger:
            try:
                query = {
                    "query": {
                        "bool": {
                            "must": [
                                {"term": {"run_id": run_id}},
                                {"term": {"artifact_type": "json"}}
                            ]
                        }
                    }
                }
                
                response = await self.opensearch_logger.client.search("cybrty-artifacts", query)
                
                if response and response.get("hits", {}).get("hits"):
                    for hit in response["hits"]["hits"]:
                        artifact = hit["_source"]
                        artifacts["json_artifacts"].append({
                            "name": artifact["artifact_name"],
                            "created_at": artifact["created_at"],
                            "reference": f"opensearch://cybrty-artifacts/{run_id}/{artifact['artifact_name']}"
                        })
                        
            except Exception as e:
                logger.error("Error listing JSON artifacts", run_id=run_id, error=str(e))
        
        # Get file artifacts from file system
        try:
            run_dir = self.base_dir / run_id
            if run_dir.exists():
                for file_path in run_dir.iterdir():
                    if file_path.is_file() and not file_path.name.endswith('.json'):
                        artifacts["file_artifacts"].append({
                            "name": file_path.name,
                            "path": str(file_path),
                            "size": file_path.stat().st_size
                        })
        except Exception as e:
            logger.error("Error listing file artifacts", run_id=run_id, error=str(e))
        
        return artifacts


class OutputParser:
    """Parse tool outputs into structured data."""
    
    @staticmethod
    def parse_nmap_xml(xml_content: str) -> Dict[str, Any]:
        """Parse Nmap XML output."""
        try:
            root = ET.fromstring(xml_content)
            hosts = []
            
            for host in root.findall('host'):
                # Get host state
                status = host.find('status')
                if status is None or status.get('state') != 'up':
                    continue
                
                # Get IP address
                address = host.find('address')
                ip = address.get('addr') if address is not None else 'unknown'
                
                # Get hostnames
                hostnames = []
                hostnames_elem = host.find('hostnames')
                if hostnames_elem is not None:
                    for hostname in hostnames_elem.findall('hostname'):
                        hostnames.append(hostname.get('name'))
                
                # Get ports
                ports = []
                ports_elem = host.find('ports')
                if ports_elem is not None:
                    for port in ports_elem.findall('port'):
                        port_id = port.get('portid')
                        protocol = port.get('protocol')
                        
                        state = port.find('state')
                        state_value = state.get('state') if state is not None else 'unknown'
                        
                        service = port.find('service')
                        service_name = service.get('name') if service is not None else 'unknown'
                        service_product = service.get('product') if service is not None else ''
                        service_version = service.get('version') if service is not None else ''
                        
                        ports.append({
                            'port': int(port_id),
                            'protocol': protocol,
                            'state': state_value,
                            'service': service_name,
                            'product': service_product,
                            'version': service_version
                        })
                
                hosts.append({
                    'ip': ip,
                    'hostnames': hostnames,
                    'ports': ports,
                    'state': 'up'
                })
            
            return {
                'hosts': hosts,
                'summary': {
                    'total_hosts': len(hosts),
                    'scan_time': datetime.utcnow().isoformat()
                }
            }
            
        except ET.ParseError as e:
            logger.error("Failed to parse Nmap XML", error=str(e))
            return {'error': f'XML parse error: {e}', 'hosts': []}
        except Exception as e:
            logger.error("Unexpected error parsing Nmap XML", error=str(e))
            return {'error': str(e), 'hosts': []}
    
    @staticmethod
    def parse_amass_output(output: str) -> Dict[str, Any]:
        """Parse Amass output."""
        subdomains = []
        lines = output.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                subdomains.append(line)
        
        return {
            'subdomains': subdomains,
            'total_found': len(subdomains),
            'scan_time': datetime.utcnow().isoformat()
        }
    
    @staticmethod
    def parse_nikto_output(output: str) -> Dict[str, Any]:
        """Parse Nikto output."""
        findings = []
        current_target = None
        
        lines = output.split('\n')
        for line in lines:
            line = line.strip()
            
            if line.startswith('+ Target IP:') or line.startswith('+ Target Host:'):
                current_target = line.split(':', 1)[1].strip()
            elif line.startswith('+') and current_target:
                # This is a finding
                findings.append({
                    'target': current_target,
                    'finding': line[1:].strip(),
                    'severity': 'info'  # Default severity
                })
        
        return {
            'findings': findings,
            'target': current_target,
            'total_findings': len(findings),
            'scan_time': datetime.utcnow().isoformat()
        }
    
    @staticmethod
    def parse_hydra_output(output: str) -> Dict[str, Any]:
        """Parse Hydra output."""
        credentials = []
        lines = output.split('\n')
        
        for line in lines:
            line = line.strip()
            if '[' in line and ']' in line and 'login:' in line:
                # Parse successful login
                parts = line.split()
                for i, part in enumerate(parts):
                    if part == 'login:' and i + 1 < len(parts):
                        login = parts[i + 1]
                    elif part == 'password:' and i + 1 < len(parts):
                        password = parts[i + 1]
                        credentials.append({
                            'username': login,
                            'password': password,
                            'service': 'unknown'
                        })
        
        return {
            'credentials': credentials,
            'total_found': len(credentials),
            'scan_time': datetime.utcnow().isoformat()
        }


def generate_step_id() -> str:
    """Generate unique step ID."""
    return str(uuid.uuid4())


def validate_network_target(target: str, allowed_networks: List[str]) -> bool:
    """Validate if target is within allowed networks."""
    import ipaddress
    
    try:
        # Handle CIDR notation
        if '/' in target:
            target_network = ipaddress.ip_network(target, strict=False)
        else:
            target_network = ipaddress.ip_network(f"{target}/32", strict=False)
        
        for allowed_net in allowed_networks:
            allowed_network = ipaddress.ip_network(allowed_net)
            if target_network.subnet_of(allowed_network) or target_network == allowed_network:
                return True
        
        return False
        
    except ipaddress.AddressValueError:
        # If it's not an IP, assume it's a hostname (allowed)
        return True
